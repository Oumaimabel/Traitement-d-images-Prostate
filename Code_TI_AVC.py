# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YUXALcbEVNg_LkISZJrl52M3DfHBxct
"""

!pip install pydicom

!pip install rgb2gray

!pip install skimage

!pip install exposure

!pip install scipy

!pip install ndimage

import pydicom
import pydicom as dicom
import numpy as np
import matplotlib.pyplot as plt
import exposure
import skimage
from skimage import exposure
import numpy as np
import cv2
import scipy
from scipy import ndimage
from skimage.color import rgb2gray

dataset=dicom.read_file("case4a_001.dcm")
print(dataset)

plt.imshow(dataset.pixel_array,plt.cm.bone)
plt.show()

print(dataset.Rows)
print(dataset.Columns)

def dicom_to_array(filename):
    d = dicom.read_file(filename)
    a = d.pixel_array
    return np.array(a)

a1 = dicom_to_array("case4a_001.dcm")
print(a1.size)

print(a1.shape)

print(np.ndarray.min(a1))
print(np.ndarray.max(a1))

hist,bins = np.histogram(a1, bins=256)
plt.figure()
plt.hist(a1)

a1_eq = skimage.exposure.equalize_hist(a1)
hist_eq,bins_eq = np.histogram(a1_eq, bins=256)
plt.figure() # créer une nouvelle figure
plt.hist(a1_eq)

fig1  = plt.figure()
plt.imshow(a1, cmap="gray", interpolation="bicubic")
plt.colorbar()
fig1.suptitle("Original + Gray colormap", fontsize=12)

fig2 = plt.figure()
plt.imshow(a1_eq, cmap="gray", interpolation="bicubic")
plt.colorbar()
fig2.suptitle("Histogram equalization + Gray colormap", fontsize=12)

part=a1[200:400,100:300]
plt.subplot(2,2,1),plt.imshow(a1,cmap = 'gray')
plt.title('image')
plt.subplot(2,2,2),plt.imshow(part,cmap = 'gray')
plt.title('la partie affiché ')

plt.show()

folder = 'case4a_001.dcm' #folder where photos are stored
fig=plt.figure(figsize=(8, 8))
columns=4
rows=5

for i in range(1,10):

    # define subplot 1-0.dcm

    #x= folder + '1-0' + str(i) + '.dcm'
    ds = pydicom.filereader.dcmread(folder)
    fig.add_subplot(rows,columns,i)
    # load image pixels
    plt.imshow(ds.pixel_array, cmap=plt.cm.bone)
plt.show()
plt.show()

a1 = dicom_to_array("case4a_001.dcm")

kernel = np.ones((5,5),np.uint8)
erosion = cv2.erode(a1,kernel,iterations = 2)
plt.imshow(erosion,cmap='gray')

dilation = cv2.dilate(a1,kernel,iterations = 1)
plt.imshow(dilation,cmap='gray')

opening = cv2.morphologyEx(a1, cv2.MORPH_OPEN, kernel)
plt.imshow(opening,cmap='gray')


# In[26]:

#Closing
closing = cv2.morphologyEx(a1, cv2.MORPH_CLOSE, kernel)
plt.imshow(closing,cmap='gray')


# In[27]:


#Morphological Gradient
gradient = cv2.morphologyEx(a1, cv2.MORPH_GRADIENT, kernel)
plt.imshow(gradient,cmap='gray')


# In[28]:


#Top Hat
tophat = cv2.morphologyEx(a1, cv2.MORPH_TOPHAT, kernel)
plt.imshow(tophat,cmap='gray')

blackhat = cv2.morphologyEx(a1, cv2.MORPH_BLACKHAT, kernel)
plt.imshow(blackhat,cmap='gray')

blurred_face = ndimage.gaussian_filter(a1, sigma=3)
very_blurred = ndimage.gaussian_filter(a1, sigma=5)
plt.figure(figsize=(11, 6))
plt.subplot(121), plt.imshow(blurred_face, cmap='gray')
plt.title('blurred_face'), plt.xticks([]), plt.yticks([])
plt.subplot(122), plt.imshow(very_blurred, cmap='gray')
plt.title('very_blurred'), plt.xticks([]), plt.yticks([])

plt.show()

plt.hist(a1.flatten(), bins=50, color='c')
plt.xlabel("Hounsfield Units (HU)")
plt.ylabel("Frequency")
plt.show()

laplacian = cv2.Laplacian(a1,cv2.CV_64F)
sobelx = cv2.Sobel(a1,cv2.CV_64F,1,0,ksize=5)
sobely = cv2.Sobel(a1,cv2.CV_64F,0,1,ksize=5)

plt.subplot(2,2,1),plt.imshow(a1,cmap = 'gray')
plt.title('Original'), plt.xticks([]), plt.yticks([])
plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')
plt.title('Laplacian'), plt.xticks([]), plt.yticks([])
plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')
plt.title('Sobel X'), plt.xticks([]), plt.yticks([])
plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')
plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])


# In[34]:


#Filtre gaussien
blurred_face = ndimage.gaussian_filter(a1, sigma=3)
very_blurred = ndimage.gaussian_filter(a1, sigma=5)
plt.figure(figsize=(11, 6))
plt.subplot(121), plt.imshow(blurred_face, cmap='gray')
plt.title('blurred_face'), plt.xticks([]), plt.yticks([])
plt.subplot(122), plt.imshow(very_blurred, cmap='gray')
plt.title('very_blurred'), plt.xticks([]), plt.yticks([])
plt.show()


# In[35]:


gray = rgb2gray(a1)
plt.imshow(gray, cmap='gray')
gray.shape
gray_r = gray.reshape(gray.shape[0] * gray.shape[1])
for i in range(gray_r.shape[0]):
        if gray_r[i] > gray_r.mean():
            gray_r[i] = 1
        else:
            gray_r[i] = 0
gray = gray_r.reshape(gray.shape[0], gray.shape[1])

plt.subplot(121), plt.imshow(a1, cmap='gray')
plt.title('Original'), plt.xticks([]), plt.yticks([])
plt.subplot(122), plt.imshow(gray, cmap='gray')
plt.title('Segmentation'), plt.xticks([]), plt.yticks([])
plt.show()


# In[36]:


#Convolution 2D
kernel = np.ones((5,5),np.float32)/25
dst = cv2.filter2D(a1,-1,kernel)
plt.subplot(121),plt.imshow(a1,'gray'),plt.title('Original')
plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(dst,'gray'),plt.title('Averaging')
plt.xticks([]), plt.yticks([])
plt.show()


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score,classification_report,precision_score,recall_score
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

data=pd.read_csv('healthcare-dataset-stroke-data.csv')
data.head(10)
## Displaying top 10 rows
data.info()
## Showing information about datase
data.describe()
## Showing data's statistical features
data.drop("id",inplace=True,axis=1)

print('Unique values\n',data['gender'].unique())
print('Value Counts\n',data['gender'].value_counts())
# Above codes will help to give us information about it's unique values and count of each value.

sns.countplot(data=data,x='gender')
# Helps to plot a count plot which will help us to see count of values in each unique category.
sns.countplot(data=data,x='gender',hue='stroke')
# This plot will help to analyze how gender will affect chances of stroke.

data['age'].nunique()
# Returns number of unique values in this attribute
sns.displot(data['age'])
# This will plot a distribution plot of variable age
plt.figure(figsize=(15,7))
sns.boxplot(data=data,x='stroke',y='age')
# Above code will plot a boxplot of variable age with respect of target attribute stroke

data['age'].nunique()
# Returns number of unique values in this attribute
sns.displot(data['age'])
# This will plot a distribution plot of variable age
plt.figure(figsize=(15,7))
sns.boxplot(data=data,x='stroke',y='age')
# Above code will plot a boxplot of variable age with respect of target attribute stroke
print('Unique Value\n',data['heart_disease'].unique())
print('Value Counts\n',data['heart_disease'].value_counts())
# Above code will return unique value for heart disease attribute and its value counts
sns.countplot(data=data,x='heart_disease')
# Will plot a counter plot of variable heart diseases
print('Unique Values\n',data['ever_married'].unique())
print('Value Counts\n',data['ever_married'].value_counts())
# Above code will show us number unique values of attribute and its count
sns.countplot(data=data,x='ever_married')
# Counter plot of ever married attribute
sns.countplot(data=data,x='ever_married',hue='stroke')
# Ever married with respect of stroke
print('Unique Value\n',data['work_type'].unique())
print('Value Counts\n',data['work_type'].value_counts())
# Above code will return unique values of attributes and its count
sns.countplot(data=data,x='work_type')
# Above code will create a count plot
sns.countplot(data=data,x='work_type',hue='stroke')
# Above code will create a count plot with respect to stroke
print('Unique Values\n',data['Residence_type'].unique())
print("Value Counts\n",data['Residence_type'].value_counts())
# Above code will return unique values of variable and its count
sns.countplot(data=data,x='Residence_type')
# This will create a counter plot
sns.countplot(data=data,x='Residence_type',hue='stroke')
# Residence Type with respect to stroke
data['avg_glucose_level'].nunique()
# Number of unique values
sns.displot(data['avg_glucose_level'])
# Distribution of avg_glucose_level
sns.boxplot(data=data,x='stroke',y='avg_glucose_level')
# Avg_glucose_level and Stroke
data['bmi'].isna().sum()
# Returns number null values
data['bmi'].fillna(data['bmi'].mean(),inplace=True)
# Filling null values with average value
data['bmi'].nunique()
# Returns number of unique values in that attribute
sns.displot(data['bmi'])
# Distribution of bmi
sns.boxplot(data=data,x='stroke',y='bmi')
# BMI with respect to Stroke
print('Unique Values\n',data['smoking_status'].unique())
print('Value Counts\n',data['smoking_status'].value_counts())
# Returns unique values and its count
sns.countplot(data=data,x='smoking_status')
# Count plot of smoking status
sns.countplot(data=data,x='smoking_status',hue='stroke')
# Smoking Status with respect to Stroke
print('Unique Value\n',data['stroke'].unique())
print('Value Counts\n',data['stroke'].value_counts())
# Returns Unique Value and its count
sns.countplot(data=data,x='stroke')
# Count Plot of Stroke
cols=data.select_dtypes(include=['object']).columns
print(cols)
# This code will fetech columns whose data type is object.
le=LabelEncoder()
# Initializing our Label Encoder object
data[cols]=data[cols].apply(le.fit_transform)
# Transfering categorical data into numeric
print(data.head(10))
Index(['gender', 'ever_married', 'work_type', 'Residence_type',
       'smoking_status']
      
classifier = SelectKBest(score_func=f_classif,k=5)
fits = classifier.fit(data.drop('stroke',axis=1),data['stroke'])
x=pd.DataFrame(fits.scores_)
columns = pd.DataFrame(data.drop('stroke',axis=1).columns)
fscores = pd.concat([columns,x],axis=1)
fscores.columns = ['Attribute','Score']
fscores.sort_values(by='Score',ascending=False)
cols=fscores[fscores['Score']>50]['Attribute']
print(cols)

train_x,test_x,train_y,test_y=train_test_split(data[cols],data['stroke'],random_state=1255,test_size=0.25)
#Splitting data
train_x.shape,test_x.shape,train_y.shape,test_y.shape
# Shape of data
smote=SMOTE()
train_x,train_y=smote.fit_resample(train_x,train_y)
te
st_x,test_y=smote.fit_resample(test_x,test_y)
print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)
# Estimate Knn model and report the outcome : 
from sklearn.neighbors import KNeighborsClassifier
# Create the model : 
knn = KNeighborsClassifier(n_neighbors=3)
# Train (fit) the model :
knn = knn.fit(x_train, y_train)
# Make a prediction at the first 5 rows regrads (k = 3) till we dertmine the optimum value of k 
y_pred = knn.predict(x_test)
y_pred[:5]
from sklearn.metrics import accuracy_score, classification_report, f1_score
print(classification_report(y_test, y_pred))
print('Accuracy Score:', round(accuracy_score(y_test, y_pred,2)))
print("F1_score: ", round(f1_score(y_test, y_pred,2)))
from sklearn.tree import DecisionTreeClassifier 
# Create the model 
dt = DecisionTreeClassifier(random_state=42)
# Fit the model
dt = dt.fit(x_train, y_train)
# Determine the number of nodes and maximum depth:
dt.tree_.node_count, dt.tree_.max_depth
train_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),
                                   measure_error(y_test, y_test_pred,'test')],axis=1)
train_test_full_error
from sklearn.model_selection import GridSearchCV
param_grid = {'max_depth':range(1, dt.tree_.max_depth+1,2),
              'max_features': range(1, len(dt.feature_importances_)+1)}

GR = GridSearchCV(DecisionTreeClassifier(random_state=42),
                  param_grid = param_grid,
                  scoring = 'accuracy',
                  n_jobs = -1)              
# Fit the Grid Search model: 
GR = GR.fit(x_train, y_train)
# Get the number of the nodes and Maximmum Depth: 
GR.best_estimator_.tree_.node_count , GR.best_estimator_.tree_.max_depth
# Make predication: 
y_train_pred_gr = GR.predict(x_train)
y_train_pred_gr[:]

y_test_pred_gr = GR.predict(x_test)
y_test_pred_gr[:5]
train_test_full_error_gr = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),
                                   measure_error(y_test, y_test_pred_gr,'test')],axis=1)
# Create a heatmap for checking variables correlations : 
fig, ax = plt.subplots(figsize=(12, 8))
sns.heatmap(df.corr())

import warnings
warnings.filterwarnings("ignore", category= UserWarning)
warnings.filterwarnings("ignore", category= RuntimeWarning)
from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier(oob_score=True,
                            random_state=42,
                            warm_start=True,
                            n_jobs = -1)

oob_list = list()
for n_trees in [15, 20, 50 , 60, 100, 110, 115, 200, 260, 300, 500, 550, 600, 670, 700, 780, 800]:
  RF.set_params(n_estimators=n_trees)

  # Fit the model : 
  RF.fit(x_train, y_train)

  # Get the oob Erros:
  oob_error = 1 - RF.oob_score_
  # Score it : 
  oob_list.append(pd.Series({'n_trees': n_trees, 'OOb':oob_error}))

rf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')  
rf_oob_df
